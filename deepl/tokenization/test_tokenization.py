#!/usr/bin/env python3
"""
Demo tokenization ƒë·ªÉ hi·ªÉu c√°ch AI x·ª≠ l√Ω token
"""

# Simulate tokenization (kh√¥ng c·∫ßn OpenAI API)
def simulate_tokenization_vietnamese():
    """M√¥ ph·ªèng c√°ch tokenize ti·∫øng Vi·ªát"""
    
    text = "AI model dung thuat toan BPE de xu ly ngon ngu"
    print(f"Original text: {text}")
    print()
    
    # Gi·∫£ l·∫≠p BPE tokenization process
    print("=== BPE TOKENIZATION PROCESS ===")
    
    # Step 1: Character level
    chars = list(text)
    print(f"Step 1 - Characters: {chars}")
    
    # Step 2: Merge common patterns (m√¥ ph·ªèng)
    # Trong th·ª±c t·∫ø, AI ƒë√£ h·ªçc ƒë∆∞·ª£c patterns n√†y t·ª´ training
    common_patterns = {
        "AI": "AI",
        " model": " model", 
        " dung": " dung",
        " thuat": " thuat",
        " toan": " toan",
        " BPE": " BPE",
        " de": " de",
        " xu": " xu",
        " ly": " ly", 
        " ngon": " ngon",
        " ngu": " ngu"
    }
    
    # Simulate merged tokens
    tokens = []
    i = 0
    while i < len(text):
        # T√¨m longest match
        found = False
        for length in range(6, 0, -1):  # T·ª´ d√†i ƒë·∫øn ng·∫Øn
            if i + length <= len(text):
                substring = text[i:i+length]
                if substring in common_patterns:
                    tokens.append(f"'{substring}'")
                    i += length
                    found = True
                    break
        if not found:
            tokens.append(f"'{text[i]}'")
            i += 1
    
    print(f"Step 2 - Tokens: {tokens}")
    print(f"Total tokens: {len(tokens)}")
    print()
    
    # Step 3: How AI understands context
    print("=== CONTEXT UNDERSTANDING ===")
    
    context_analysis = {
        "'AI'": "Technical term - Machine Learning context",
        "' model'": "ML concept - follows AI", 
        "' dung'": "Vietnamese verb - 'use/utilize'",
        "' thuat'": "Part of compound 'thu·∫≠t to√°n' (algorithm)",
        "' toan'": "Completes 'thu·∫≠t to√°n' concept",
        "' BPE'": "Technical acronym - Byte Pair Encoding",
        "' de'": "Vietnamese preposition - 'to/for'",
        "' xu'": "Start of 'x·ª≠ l√Ω' (process)", 
        "' ly'": "Completes 'x·ª≠ l√Ω' concept",
        "' ngon'": "Start of 'ng√¥n ng·ªØ' (language)",
        "' ngu'": "Completes 'ng√¥n ng·ªØ' concept"
    }
    
    for token, meaning in context_analysis.items():
        print(f"{token:12} ‚Üí {meaning}")
    
    print()
    print("=== AI SEMANTIC UNDERSTANDING ===")
    print("Full context interpretation:")
    print("- 'AI model' ‚Üí Machine Learning model")
    print("- 'dung thuat toan' ‚Üí uses algorithm") 
    print("- 'BPE' ‚Üí Byte Pair Encoding technique")
    print("- 'de xu ly ngon ngu' ‚Üí to process language")
    print()
    print("üéØ Final understanding: AI model uses BPE algorithm to process language")

def demonstrate_subword_patterns():
    """Minh h·ªça c√°ch AI nh·∫≠n bi·∫øt patterns trong subwords"""
    
    print("\n=== SUBWORD PATTERN RECOGNITION ===")
    
    examples = {
        "running": ["run", "ning"],
        "unhappy": ["un", "happy"], 
        "tokenization": ["token", "ization"],
        "preprocessing": ["pre", "process", "ing"],
        "thu·∫≠t to√°n": ["thu·∫≠t", " to√°n"],
        "x·ª≠ l√Ω": ["x·ª≠", " l√Ω"],
        "ng√¥n ng·ªØ": ["ng√¥n", " ng·ªØ"]
    }
    
    for word, tokens in examples.items():
        print(f"'{word}' ‚Üí {tokens}")
        
        # Analyze semantic components
        if word == "running":
            print("  Semantic: ROOT(run) + SUFFIX(ning) = continuous action")
        elif word == "unhappy": 
            print("  Semantic: PREFIX(un) + ROOT(happy) = negation")
        elif word == "tokenization":
            print("  Semantic: ROOT(token) + SUFFIX(ization) = process of making tokens")
        elif word == "thu·∫≠t to√°n":
            print("  Semantic: thu·∫≠t(technique) + to√°n(math) = algorithm")
        elif word == "x·ª≠ l√Ω":
            print("  Semantic: x·ª≠(handle) + l√Ω(logic) = process")
        elif word == "ng√¥n ng·ªØ":
            print("  Semantic: ng√¥n(speech) + ng·ªØ(language) = language")
        print()

def show_context_vectors():
    """M√¥ ph·ªèng c√°ch AI d√πng vector ƒë·ªÉ hi·ªÉu context"""
    
    print("=== CONTEXT VECTOR SIMULATION ===")
    print("(Simplified representation of how AI encodes meaning)")
    print()
    
    # Gi·∫£ l·∫≠p vector representations (th·ª±c t·∫ø c√≥ 768-4096 dimensions)
    token_vectors = {
        "AI": [0.8, 0.2, -0.1, 0.9, 0.3],      # High tech, positive context
        "model": [0.7, 0.5, 0.2, 0.8, 0.1],   # Mathematical, structured
        "dung": [0.1, -0.2, 0.6, 0.3, 0.4],   # Action verb, Vietnamese
        "thuat": [0.3, 0.4, 0.5, 0.7, 0.6],   # Technical term prefix
        "toan": [0.5, 0.7, 0.4, 0.9, 0.8],    # Mathematics, completes "thuat"
        "BPE": [0.9, 0.1, -0.3, 0.8, 0.7]     # Technical acronym, NLP
    }
    
    print("Token vectors (5-dim simplified):")
    for token, vector in token_vectors.items():
        print(f"{token:8} ‚Üí {vector}")
    
    print()
    print("Context relationships (cosine similarity simulation):")
    print("- 'AI' ‚Üî 'model': 0.92 (high similarity - related concepts)")
    print("- 'thuat' ‚Üî 'toan': 0.87 (high - forms compound word)")  
    print("- 'AI' ‚Üî 'BPE': 0.78 (medium-high - both tech terms)")
    print("- 'dung' ‚Üî 'model': 0.45 (medium - verb-object relationship)")
    print()
    print("üß† AI combines these relationships to understand full meaning!")

if __name__ == "__main__":
    simulate_tokenization_vietnamese()
    demonstrate_subword_patterns() 
    show_context_vectors()
